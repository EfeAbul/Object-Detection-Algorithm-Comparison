{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "z = zipfile.ZipFile('/content/SSD_Data.zip')\n",
        "extract_path = \"/content/dataset\"\n",
        "z.extractall(extract_path)"
      ],
      "metadata": {
        "id": "VBseC9v5q_1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.models.detection.ssd import SSDClassificationHead\n",
        "from torchvision.models.detection import _utils\n",
        "from torchvision.models.detection import SSD300_VGG16_Weights"
      ],
      "metadata": {
        "id": "l4rtsyeUs9nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotation_dirs = [\"/content/dataset/train/labels\"]\n",
        "images_dirs = [\"/content/dataset/train/images\"]\n",
        "\n",
        "print(f'Before : {len(os.listdir(annotation_dirs[0]))}')\n",
        "\n",
        "for annotation_dir, images_dir in zip(annotation_dirs, images_dirs):\n",
        "    annotations = os.listdir(annotation_dir)\n",
        "    images = set(os.listdir(images_dir))  # Use a set for faster lookup\n",
        "\n",
        "    for annotation in annotations:\n",
        "        annotation_file = os.path.join(annotation_dir, annotation)\n",
        "        image_file_name = annotation[:-4] + '.jpg'\n",
        "\n",
        "        # Delete annotation file if the corresponding image is missing\n",
        "        if image_file_name not in images:\n",
        "            os.remove(annotation_file)\n",
        "            continue  # No need to check further\n",
        "\n",
        "        with open (annotation_file,\"r\") as f:\n",
        "            for line in f:\n",
        "                data = line.strip().split()\n",
        "                if len(data) < 5:\n",
        "                    f.close()\n",
        "                    os.remove(annotation_file)\n",
        "                break\n",
        "\n",
        "print(f'After : {len(os.listdir(annotation_dirs[0]))}')"
      ],
      "metadata": {
        "id": "cqaOhZJjtz2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset():\n",
        "    def __init__(self, image_dir, annotations, annotations_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.annotations = annotations\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get annotation file name\n",
        "        annotation = self.annotations[idx]\n",
        "        # Get image path\n",
        "        img_path = os.path.join(self.image_dir, annotation[:-4] + '.jpg')\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image {img_path} not found.\")\n",
        "\n",
        "        # Convert image channels\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get image dimensions\n",
        "        height, width, _ = image.shape\n",
        "\n",
        "        # Parse the annotation file\n",
        "        annotation_file = os.path.join(self.annotations_dir, annotation)\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        # Open annotation file\n",
        "        with open(annotation_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                # Split data\n",
        "                data = line.strip().split()\n",
        "                class_index = int(data[0])  # Class index\n",
        "                x_center = float(data[1]) * width\n",
        "                y_center = float(data[2]) * height\n",
        "                box_width = float(data[3]) * width\n",
        "                box_height = float(data[4]) * height\n",
        "\n",
        "                # Convert to absolute coordinates [xmin, ymin, xmax, ymax]\n",
        "                xmin = x_center - box_width / 2\n",
        "                ymin = y_center - box_height / 2\n",
        "                xmax = x_center + box_width / 2\n",
        "                ymax = y_center + box_height / 2\n",
        "\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(int(class_index + 1))\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Create target dictionary\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "Hw2GtZJTuKMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create the SSD model\n",
        "def create_model(num_classes=6, size=300):\n",
        "    # Load a pre-trained SSD model (SSD300 with VGG16 backbone)\n",
        "    model = torchvision.models.detection.ssd300_vgg16(weights=SSD300_VGG16_Weights.COCO_V1)\n",
        "    # Explanation: The `ssd300_vgg16` model is a version of SSD with a VGG16 backbone, pre-trained on the COCO dataset.\n",
        "\n",
        "    # print(model)\n",
        "\n",
        "    # Retrieve the number of output channels from the backbone given the input size\n",
        "    in_channels = _utils.retrieve_out_channels(model.backbone, (size, size))\n",
        "    # Explanation: This gets the number of channels coming out of the backbone by passing the desired input size. This is important for modifying later layers.\n",
        "\n",
        "    # print(\"inchannels ==> \\n\\n\",in_channels)\n",
        "\n",
        "    # Get the number of anchors per location from the model's anchor generator\n",
        "    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
        "    # Explanation: SSD uses anchor boxes at different aspect ratios and scales. This line retrieves the number of anchors generated by the model.\n",
        "\n",
        "    # print(\"anchors ==> \\n\\n\",num_anchors)\n",
        "\n",
        "    # Replace the classification head to accommodate the number of classes\n",
        "    model.head.classification_head = SSDClassificationHead(\n",
        "        in_channels=in_channels,\n",
        "        num_anchors=num_anchors,\n",
        "        num_classes=num_classes\n",
        "    )\n",
        "    # Explanation: The `classification_head` is replaced with a new one that matches the number of classes defined by `num_classes`. It uses the number of channels from the backbone and the number of anchors.\n",
        "\n",
        "    # Adjust the transform sizes to match the input size\n",
        "    model.transform.min_size = (size,)\n",
        "    model.transform.max_size = size\n",
        "    # Explanation: These lines modify the model's transform settings to adjust the input image sizes. `min_size` and `max_size` determine the resizing behavior.\n",
        "\n",
        "    # Return the modified model\n",
        "    return model\n",
        "\n",
        "# Main block to initialize and print the model\n",
        "model = create_model(num_classes=6, size=640)\n",
        "# Explanation: This line creates a model with 2 classes (e.g., pothole and background) and an input image size of 640x640 pixels.\n",
        "\n",
        "# print(model)\n",
        "# Explanation: This prints the architecture of the created model."
      ],
      "metadata": {
        "id": "lYUVBG6AuOHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "9f1D6k3BuZay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Dataset and DataLoader\n",
        "\n",
        "## TRAIN DATALOADER\n",
        "train_image_dir = \"/content/dataset/train/images\"\n",
        "train_annotation_dir = \"/content/dataset/train/labels\"\n",
        "train_annotations = os.listdir(train_annotation_dir)\n",
        "\n",
        "train_dataset = CustomDataset(train_image_dir, train_annotations, train_annotation_dir, transform=transforms.ToTensor())\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "CMt-CZu1udRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Örnek test data loader (seninki farklı olabilir)\n",
        "test_image_dir = \"/content/dataset/test/images\"\n",
        "test_annotation_dir = \"/content/dataset/test/labels\"\n",
        "test_annotations = os.listdir(test_annotation_dir)\n",
        "\n",
        "test_dataset = CustomDataset(test_image_dir, test_annotations, test_annotation_dir, transform=transforms.ToTensor())\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "l8wRxfPhf68Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set yolunu belirt\n",
        "val_image_dir = \"/content/dataset/valid/images\"\n",
        "val_annotation_dir = \"/content/dataset/valid/labels\"\n",
        "val_annotations = os.listdir(val_annotation_dir)\n",
        "\n",
        "# Dataset ve DataLoader\n",
        "val_dataset = CustomDataset(val_image_dir, val_annotations, val_annotation_dir, transform=transforms.ToTensor())\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "Lar224Kzhk2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data_loader))\n",
        "print(len(test_data_loader))"
      ],
      "metadata": {
        "id": "W9D9hZghus0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "NKJmYoPqu1fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
        "ax = ax.ravel()\n",
        "\n",
        "# Get a batch of images and targets from the data loader\n",
        "data_loader_iter = iter(train_data_loader)\n",
        "images, targets = next(data_loader_iter)\n",
        "\n",
        "\n",
        "class_names = ['bg', 'ambulance', 'bus', 'car', 'motorcycle', 'truck']\n",
        "\n",
        "# Plot the images with bounding boxes\n",
        "for idx in range(2):\n",
        "    image = images[idx].permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C) for visualization\n",
        "    target = targets[idx]\n",
        "    boxes = target[\"boxes\"].numpy()\n",
        "    labels = target[\"labels\"].numpy()\n",
        "\n",
        "    ax[idx].imshow(image)\n",
        "\n",
        "    # Add bounding boxes to the image\n",
        "    for box, label in zip(boxes, labels):\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        width, height = xmax - xmin, ymax - ymin\n",
        "        rect = patches.Rectangle(\n",
        "            (xmin, ymin),\n",
        "            width,\n",
        "            height,\n",
        "            linewidth=2,\n",
        "            edgecolor=\"red\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        ax[idx].add_patch(rect)\n",
        "        ax[idx].text(\n",
        "            xmin,\n",
        "            ymin - 10,\n",
        "            f\"{class_names[label]}\",\n",
        "            color=\"red\",\n",
        "            fontsize=12,\n",
        "            bbox=dict(facecolor=\"yellow\", alpha=0.5, edgecolor=\"red\"),\n",
        "        )\n",
        "\n",
        "# Display the image with bounding boxes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vxlc1Gosu_7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import torchvision.ops as ops\n",
        "\n",
        "def evaluate_model(model, data_loader, device, iou_threshold=0.5, score_threshold=0.5):\n",
        "    model.eval()\n",
        "    total_tp = defaultdict(int)\n",
        "    total_fp = defaultdict(int)\n",
        "    total_fn = defaultdict(int)\n",
        "    ap_per_class = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            outputs = model(images)\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                if len(target[\"boxes\"]) == 0:\n",
        "                    continue\n",
        "\n",
        "                pred_boxes = output['boxes']\n",
        "                pred_labels = output['labels']\n",
        "                pred_scores = output['scores']\n",
        "\n",
        "                gt_boxes = target[\"boxes\"].to(device)\n",
        "                gt_labels = target[\"labels\"].to(device)\n",
        "\n",
        "                keep = pred_scores > score_threshold\n",
        "                pred_boxes = pred_boxes[keep]\n",
        "                pred_labels = pred_labels[keep]\n",
        "\n",
        "                matched_gt = set()\n",
        "                for i in range(len(pred_boxes)):\n",
        "                    box = pred_boxes[i].unsqueeze(0)\n",
        "                    label = pred_labels[i].item()\n",
        "                    ious = ops.box_iou(box, gt_boxes)\n",
        "                    iou_max, idx = ious.max(1)\n",
        "                    idx = idx.item()\n",
        "\n",
        "                    if iou_max.item() >= iou_threshold and gt_labels[idx].item() == label and idx not in matched_gt:\n",
        "                        total_tp[label] += 1\n",
        "                        matched_gt.add(idx)\n",
        "                        ap_per_class[label].append(iou_max.item())\n",
        "                    else:\n",
        "                        total_fp[label] += 1\n",
        "\n",
        "                for i, lbl in enumerate(gt_labels):\n",
        "                    if i not in matched_gt:\n",
        "                        total_fn[lbl.item()] += 1\n",
        "\n",
        "    total_p, total_r, total_ap50, total_ap5095, count = 0, 0, 0, 0, 0\n",
        "\n",
        "    for label in total_tp:\n",
        "        tp = total_tp[label]\n",
        "        fp = total_fp[label]\n",
        "        fn = total_fn[label]\n",
        "        p = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        aps = ap_per_class[label]\n",
        "        ap50 = sum([1 if ap > 0.5 else 0 for ap in aps]) / len(aps) if aps else 0\n",
        "        ap5095 = sum(aps) / len(aps) if aps else 0\n",
        "        total_p += p\n",
        "        total_r += r\n",
        "        total_ap50 += ap50\n",
        "        total_ap5095 += ap5095\n",
        "        count += 1\n",
        "\n",
        "    avg_p = total_p / count\n",
        "    avg_r = total_r / count\n",
        "    avg_ap50 = total_ap50 / count\n",
        "    avg_ap5095 = total_ap5095 / count\n",
        "\n",
        "    val_precisions.append(avg_p)\n",
        "    val_recalls.append(avg_r)\n",
        "    val_map50.append(avg_ap50)\n",
        "    val_map5095.append(avg_ap5095)\n",
        "\n",
        "    print(f\"Precision: {avg_p:.3f}, Recall: {avg_r:.3f}, mAP50: {avg_ap50:.3f}, mAP50-95: {avg_ap5095:.3f}\")\n"
      ],
      "metadata": {
        "id": "_wiOwlUVuTgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "val_map50 = []\n",
        "val_map5095 = []\n",
        "epochs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_running_loss = 0\n",
        "\n",
        "    for images, targets in train_data_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_running_loss += losses.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    epoch_loss = train_running_loss / len(train_dataset)\n",
        "    train_loss_list.append(epoch_loss)\n",
        "    epochs.append(epoch + 1)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Değerlendirme\n",
        "    evaluate_model(model, val_data_loader, device)\n",
        "    torch.save(model.state_dict(), f\"ssd_epoch_{epoch+1}.pth\")\n"
      ],
      "metadata": {
        "id": "EekMXp0Jud7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "metrics = [\n",
        "    train_loss_list, val_precisions, val_recalls,\n",
        "    val_map50, val_map5095\n",
        "]\n",
        "\n",
        "titles = [\n",
        "    \"train/loss\", \"val/precision\", \"val/recall\",\n",
        "    \"metrics/mAP50\", \"metrics/mAP50-95\"\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, data in enumerate(metrics):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.plot(data, label='value', marker='o')\n",
        "    if len(data) >= 5:\n",
        "        plt.plot(pd.Series(data).rolling(3).mean(), linestyle='dotted', label='smooth')\n",
        "    plt.title(titles[i])\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i7KJQy2Kx-oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_loss_list, marker='o', color='b')\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5hFtLrnOnjzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"SSD_custom1.pth\")"
      ],
      "metadata": {
        "id": "4xX7_jo2wr4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "# Step 1: Recreate the model architecture\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = create_model(num_classes=6, size=640)\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"/content/ssd_epoch_10.pth\"))\n",
        "# Assuming `model` and `test_data_loader` are already defined\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "model.to(device)  # Ensure the model is on the correct device\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(4, 4, figsize=(20,20))\n",
        "ax = ax.ravel()\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "test_dir = \"/content/dataset/test/images\"\n",
        "test_list_file = os.listdir(test_dir)\n",
        "# font = ImageFont.truetype(\"arial.ttf\", 28)\n",
        "\n",
        "\n",
        "class_names = ['bg', 'ambulance', 'bus', 'car', 'motorcycle', 'truck']\n",
        "\n",
        "for idx in range(16):\n",
        "\n",
        "    # Prepare the image\n",
        "    img_path = os.path.join(test_dir,random.choice(test_list_file))\n",
        "    image = Image.open(img_path)\n",
        "    image = transform(image)\n",
        "    image_tensor = image.unsqueeze(0).to(device)  # Add a batch dimension\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_tensor)[0]  # Get the first image's predictions\n",
        "\n",
        "    # print(prediction)\n",
        "\n",
        "    # Convert image to numpy for visualization\n",
        "    image_np = image.cpu().permute(1, 2, 0).numpy()  # Convert (C, H, W) -> (H, W, C)\n",
        "\n",
        "    # Extract predictions\n",
        "    boxes = prediction[\"boxes\"].cpu().numpy()\n",
        "    labels = prediction[\"labels\"].cpu().numpy()\n",
        "    scores = prediction[\"scores\"].cpu().numpy()\n",
        "\n",
        "    ax[idx].imshow(image_np)\n",
        "    ax[idx].axis(\"off\")\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    colors = [\n",
        "        (1, 0, 0),       # Red\n",
        "        (0, 1, 0),       # Green\n",
        "        (0, 0, 1),       # Blue\n",
        "        (1, 0, 1),       # Magenta\n",
        "        (1, 1, 0),       # Yellow\n",
        "        (0, 1, 1),       # Cyan\n",
        "        (1, 0.55, 0),    # Orange\n",
        "        (0, 0.5, 0)      # Dark Green\n",
        "    ]\n",
        "\n",
        "    # Add bounding boxes to the image\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        color = random.choice(colors)\n",
        "        if score >= 0.5:\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            width, height = xmax - xmin, ymax - ymin\n",
        "            rect = patches.Rectangle(\n",
        "                (xmin, ymin),\n",
        "                width,\n",
        "                height,\n",
        "                linewidth=2,\n",
        "                edgecolor=color,\n",
        "                facecolor=\"none\",\n",
        "            )\n",
        "            ax[idx].add_patch(rect)\n",
        "            ax[idx].text(\n",
        "                xmin,\n",
        "                ymin - 8,\n",
        "                f\"{class_names[label]}\",\n",
        "                color='w',\n",
        "                fontsize=12,\n",
        "                bbox=dict(facecolor=color, alpha=0.5, edgecolor=color),\n",
        "            )\n",
        "\n",
        "# Display the image with bounding boxes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nY6_XWe9AWF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Sınıf isimleri (bg hariç!)\n",
        "class_names = ['ambulance', 'bus', 'car', 'motorcycle', 'truck']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Cihaz seçimi\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# SSD modelini oluştur ve eğitilmiş ağırlıkları yükle\n",
        "from torchvision.models.detection.ssd import ssd300_vgg16\n",
        "model = ssd300_vgg16(weights=None, num_classes=num_classes + 1)  # +1 çünkü bg dahil\n",
        "model.load_state_dict(torch.load('/content/ssd_epoch_10.pth', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Görsel ön işleme\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((300, 300)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Klasör yolları\n",
        "image_folder = '/content/dataset/valid/images'\n",
        "label_folder = '/content/dataset/valid/labels'\n",
        "\n",
        "# Tahmin ve gerçek etiket listeleri\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Label okuma fonksiyonu (YOLO formatı)\n",
        "def load_labels(label_path):\n",
        "    with open(label_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        class_indices = [int(line.strip().split()[0]) for line in lines]\n",
        "    return class_indices\n",
        "\n",
        "# Görseller üzerinde tahmin ve karşılaştırma\n",
        "for image_name in os.listdir(image_folder):\n",
        "    if not image_name.endswith(('.jpg', '.png', '.jpeg')):\n",
        "        continue\n",
        "\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "    label_path = os.path.join(label_folder, image_name.rsplit('.', 1)[0] + '.txt')\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)[0]\n",
        "\n",
        "    scores = outputs['scores']\n",
        "    labels = outputs['labels']  # Bunlar modelin output sınıf id'leri\n",
        "\n",
        "    # Gerçek sınıfı oku\n",
        "    true_classes = load_labels(label_path)\n",
        "\n",
        "    # Tahmin edilen sınıf (confidence > 0.5 olan en yüksek skorlu sınıf)\n",
        "    if len(scores) > 0 and scores[0] > 0.5:\n",
        "        pred_class = labels[0].item()\n",
        "        if pred_class != 0:  # bg değilse\n",
        "            y_pred.append(pred_class - 1)  # bg'yi çıkar (0-based)\n",
        "            if len(true_classes) > 0:\n",
        "                y_true.append(true_classes[0])\n",
        "    elif len(true_classes) > 0:\n",
        "        # Model hiçbir şey tahmin etmediyse ama aslında etiket varsa, boş tahmin say\n",
        "        y_pred.append(-1)  # yanlış negatif\n",
        "        y_true.append(true_classes[0])\n",
        "\n",
        "# -1 tahminleri filtrele\n",
        "filtered_true = []\n",
        "filtered_pred = []\n",
        "for t, p in zip(y_true, y_pred):\n",
        "    if p != -1:\n",
        "        filtered_true.append(t)\n",
        "        filtered_pred.append(p)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(filtered_true, filtered_pred, labels=list(range(num_classes)))\n",
        "\n",
        "# Heatmap görselleştirme\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix Heatmap for SSD Model (bg hariç)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KcoAEko03F6H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}